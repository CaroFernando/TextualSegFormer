{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from ZeroShotDataset import ZeroShotDataset\n",
    "from params import *\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from LossFunc import *\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.utils.data import random_split\n",
    "from CLIPConditionedSegFormerModel import CLIPConditionedSegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TrainParams.TRAIN_CSV_PATH)\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>mask</th>\n",
       "      <th>label</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_0.jpg</td>\n",
       "      <td>hot dog</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_1.jpg</td>\n",
       "      <td>bottle</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_2.jpg</td>\n",
       "      <td>cup</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_3.jpg</td>\n",
       "      <td>person</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_4.jpg</td>\n",
       "      <td>spoon</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973173</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973173.jpg</td>\n",
       "      <td>bush</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973174</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973174.jpg</td>\n",
       "      <td>cage</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973175</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973175.jpg</td>\n",
       "      <td>clouds</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973176</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973176.jpg</td>\n",
       "      <td>grass</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973177</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973177.jpg</td>\n",
       "      <td>tree</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>973178 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   image                     mask    label  category_id\n",
       "0       000000558840.jpg       000000558840_0.jpg  hot dog           58\n",
       "1       000000558840.jpg       000000558840_1.jpg   bottle           44\n",
       "2       000000558840.jpg       000000558840_2.jpg      cup           47\n",
       "3       000000558840.jpg       000000558840_3.jpg   person            1\n",
       "4       000000558840.jpg       000000558840_4.jpg    spoon           50\n",
       "...                  ...                      ...      ...          ...\n",
       "973173  000000581929.jpg  000000581929_973173.jpg     bush           97\n",
       "973174  000000581929.jpg  000000581929_973174.jpg     cage           99\n",
       "973175  000000581929.jpg  000000581929_973175.jpg   clouds          106\n",
       "973176  000000581929.jpg  000000581929_973176.jpg    grass          124\n",
       "973177  000000581929.jpg  000000581929_973177.jpg     tree          169\n",
       "\n",
       "[973178 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hot dog     121\n",
       "cabinet     121\n",
       "house       121\n",
       "platform    121\n",
       "railroad    121\n",
       "           ... \n",
       "broccoli    121\n",
       "mouse       121\n",
       "keyboard    121\n",
       "horse       121\n",
       "moss        121\n",
       "Name: label, Length: 171, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_freqs = train_df[\"label\"].value_counts()\n",
    "min_freq = label_freqs.min()\n",
    "max_freq = label_freqs.max()\n",
    "\n",
    "balanced_train_df = pd.DataFrame(columns=train_df.columns)\n",
    "for label in train_df[\"label\"].unique():\n",
    "    balanced_train_df = balanced_train_df.append(train_df[train_df[\"label\"] == label].sample(min_freq), ignore_index=True)\n",
    "\n",
    "balanced_train_df[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedDatasetStuff512/images/train/ ProcessedDatasetStuff512/masks/train/\n"
     ]
    }
   ],
   "source": [
    "print(TrainParams.DATASET_IMAGE_FOLDER_TRAIN, TrainParams.DATASET_MASK_FOLDER_TRAIN,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ZeroShotDataset(\n",
    "    df = balanced_train_df, \n",
    "    image_folder = TrainParams.DATASET_IMAGE_FOLDER_TRAIN,\n",
    "    mask_folder = TrainParams.DATASET_MASK_FOLDER_TRAIN,\n",
    "    image_size = TrainParams.IMAGE_DIM,\n",
    "    mask_size = TrainParams.MASK_SIZE,\n",
    "    templates = TrainParams.TEMPLATES, \n",
    "    unseen_classes = TrainParams.UNSEEN_CLASSES, \n",
    "    image_processor = clip_processor, \n",
    "    tokenizer = clip_processor.tokenizer, \n",
    "    filter_unseen = False,\n",
    "    filter_seen = True\n",
    ")\n",
    "\n",
    "val_dataset = ZeroShotDataset(\n",
    "    df = balanced_train_df, \n",
    "    image_folder = TrainParams.DATASET_IMAGE_FOLDER_TRAIN,\n",
    "    mask_folder = TrainParams.DATASET_MASK_FOLDER_TRAIN,\n",
    "    image_size = TrainParams.IMAGE_DIM,\n",
    "    mask_size = TrainParams.MASK_SIZE,\n",
    "    templates = TrainParams.TEMPLATES, \n",
    "    unseen_classes = TrainParams.UNSEEN_CLASSES, \n",
    "    image_processor = clip_processor, \n",
    "    tokenizer = clip_processor.tokenizer, \n",
    "    filter_unseen = True,\n",
    "    filter_seen = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Num workers: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch size:\", TrainParams.BATCH_SIZE)\n",
    "print(\"Num workers:\", TrainParams.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 18997\n",
      "Number of val images: 1694\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_dataset)}\")   \n",
    "print(f\"Number of val images: {len(val_dataset)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TrainParams.BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, num_workers=TrainParams.NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=TrainParams.BATCH_SIZE, shuffle=False, collate_fn=val_dataset.collate_fn, num_workers=TrainParams.NUM_WORKERS)\n",
    "test_model = CLIPConditionedSegFormer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_iou',\n",
    "    filename='transformer-{epoch:02d}-{val_loss:.3f}-{val_iou:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    "    # dirpath='checkpoints/',\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=30,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        LearningRateMonitor(logging_interval='step')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                 | Params\n",
      "---------------------------------------------------\n",
      "0 | clip      | CLIPModel            | 149 M \n",
      "1 | segformer | ConditionedSegFormer | 18.5 M\n",
      "2 | neloss    | NELoss               | 0     \n",
      "3 | acc       | Accuracy             | 0     \n",
      "4 | dice      | DiceLoss             | 0     \n",
      "5 | iou       | IoULoss              | 0     \n",
      "6 | f1score   | F1Score              | 0     \n",
      "---------------------------------------------------\n",
      "18.5 M    Trainable params\n",
      "149 M     Non-trainable params\n",
      "168 M     Total params\n",
      "672.563   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5f0289fbe84506b0f84cd561909b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c76cefebb4b4c91a25e5201217769ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(test_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
