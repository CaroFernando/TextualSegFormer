{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from ZeroShotDataset import ZeroShotDataset\n",
    "from params import *\n",
    "from DatasetModeling import *\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from LossFunc import *\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.utils.data import random_split\n",
    "from CLIPConditionedSegFormerModel import CLIPConditionedSegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TrainParams.TRAIN_CSV_PATH)\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>mask</th>\n",
       "      <th>label</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_0.jpg</td>\n",
       "      <td>hot dog</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_1.jpg</td>\n",
       "      <td>bottle</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_2.jpg</td>\n",
       "      <td>cup</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_3.jpg</td>\n",
       "      <td>person</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000558840.jpg</td>\n",
       "      <td>000000558840_4.jpg</td>\n",
       "      <td>spoon</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973173</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973173.jpg</td>\n",
       "      <td>bush</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973174</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973174.jpg</td>\n",
       "      <td>cage</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973175</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973175.jpg</td>\n",
       "      <td>clouds</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973176</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973176.jpg</td>\n",
       "      <td>grass</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973177</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "      <td>000000581929_973177.jpg</td>\n",
       "      <td>tree</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>973178 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   image                     mask    label  category_id\n",
       "0       000000558840.jpg       000000558840_0.jpg  hot dog           58\n",
       "1       000000558840.jpg       000000558840_1.jpg   bottle           44\n",
       "2       000000558840.jpg       000000558840_2.jpg      cup           47\n",
       "3       000000558840.jpg       000000558840_3.jpg   person            1\n",
       "4       000000558840.jpg       000000558840_4.jpg    spoon           50\n",
       "...                  ...                      ...      ...          ...\n",
       "973173  000000581929.jpg  000000581929_973173.jpg     bush           97\n",
       "973174  000000581929.jpg  000000581929_973174.jpg     cage           99\n",
       "973175  000000581929.jpg  000000581929_973175.jpg   clouds          106\n",
       "973176  000000581929.jpg  000000581929_973176.jpg    grass          124\n",
       "973177  000000581929.jpg  000000581929_973177.jpg     tree          169\n",
       "\n",
       "[973178 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 64115\n",
      "35012 3388\n",
      "121 242 240.83625730994152\n"
     ]
    }
   ],
   "source": [
    "balanced_train_df = balance_dataset(train_df, ratio=2)\n",
    "\n",
    "# inductive_dataset_train = inductive_dataset(balanced_train_df, TrainParams.UNSEEN_CLASSES)\n",
    "# inductive_dataset_val = inductive_dataset(balanced_train_df, TrainParams.SEEN_CLASSES)\n",
    "\n",
    "# print(len(inductive_dataset_train), len(inductive_dataset_val))\n",
    "\n",
    "transductive_dataset_train = transductive_dataset(balanced_train_df, TrainParams.SEEN_CLASSES)\n",
    "transductive_dataset_val = transductive_dataset(balanced_train_df, TrainParams.UNSEEN_CLASSES)\n",
    "\n",
    "print(len(transductive_dataset_train), len(transductive_dataset_val))\n",
    "\n",
    "# print min and max frequencies\n",
    "label_freqs = balanced_train_df[\"label\"].value_counts()\n",
    "print(label_freqs.min(), label_freqs.max(), label_freqs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessedDatasetStuff512/images/train/ ProcessedDatasetStuff512/masks/train/\n"
     ]
    }
   ],
   "source": [
    "print(TrainParams.DATASET_IMAGE_FOLDER_TRAIN, TrainParams.DATASET_MASK_FOLDER_TRAIN,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ZeroShotDataset(\n",
    "    # df = inductive_dataset_train, \n",
    "    df = transductive_dataset_train,\n",
    "    image_folder = TrainParams.DATASET_IMAGE_FOLDER_TRAIN,\n",
    "    mask_folder = TrainParams.DATASET_MASK_FOLDER_TRAIN,\n",
    "    image_size = TrainParams.IMAGE_DIM,\n",
    "    mask_size = TrainParams.MASK_SIZE,\n",
    "    templates = TrainParams.TEMPLATES, \n",
    "    unseen_classes = TrainParams.UNSEEN_CLASSES, \n",
    "    image_processor = clip_processor, \n",
    "    tokenizer = clip_processor.tokenizer, \n",
    "    filter_unseen = False,\n",
    "    filter_seen = False # True\n",
    ")\n",
    "\n",
    "val_dataset = ZeroShotDataset(\n",
    "    # df = inductive_dataset_val, \n",
    "    df = transductive_dataset_val,\n",
    "    image_folder = TrainParams.DATASET_IMAGE_FOLDER_TRAIN,\n",
    "    mask_folder = TrainParams.DATASET_MASK_FOLDER_TRAIN,\n",
    "    image_size = TrainParams.IMAGE_DIM,\n",
    "    mask_size = TrainParams.MASK_SIZE,\n",
    "    templates = TrainParams.TEMPLATES, \n",
    "    unseen_classes = TrainParams.UNSEEN_CLASSES, \n",
    "    image_processor = clip_processor, \n",
    "    tokenizer = clip_processor.tokenizer, \n",
    "    filter_unseen = False, # True\n",
    "    filter_seen = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Num workers: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch size:\", TrainParams.BATCH_SIZE)\n",
    "print(\"Num workers:\", TrainParams.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 35012\n",
      "Number of val images: 3388\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_dataset)}\")   \n",
    "print(f\"Number of val images: {len(val_dataset)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TrainParams.BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, num_workers=TrainParams.NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=TrainParams.BATCH_SIZE, shuffle=False, collate_fn=val_dataset.collate_fn, num_workers=TrainParams.NUM_WORKERS)\n",
    "test_model = CLIPConditionedSegFormer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_iou',\n",
    "    filename='transformer-{epoch:02d}-{val_loss:.3f}-{val_iou:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    "    # dirpath='checkpoints/',\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=20,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        LearningRateMonitor(logging_interval='step')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                 | Params\n",
      "---------------------------------------------------\n",
      "0 | clip      | CLIPModel            | 149 M \n",
      "1 | segformer | ConditionedSegFormer | 13.4 M\n",
      "2 | neloss    | NELoss               | 0     \n",
      "3 | acc       | Accuracy             | 0     \n",
      "4 | dice      | DiceLoss             | 0     \n",
      "5 | iou       | IoULoss              | 0     \n",
      "6 | f1score   | F1Score              | 0     \n",
      "---------------------------------------------------\n",
      "13.4 M    Trainable params\n",
      "149 M     Non-trainable params\n",
      "163 M     Total params\n",
      "652.183   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7571154b6e8d419c9937fa2e720010eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fe5808ff0748cb80624693fc61610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012bc075b662494d84d10457a4d45d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 4377: 'val_iou' reached 0.11182 (best 0.11182), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=00-val_loss=0.472-val_iou=0.11.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23014d909e5d463ab6623888219d21e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 8754: 'val_iou' reached 0.14373 (best 0.14373), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=01-val_loss=0.455-val_iou=0.14.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca863fb1e2a4df689675a7a2039f56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 13131: 'val_iou' reached 0.12440 (best 0.14373), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=02-val_loss=0.483-val_iou=0.12.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886b3351c9284df18b35d2d1bfd1e94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 17508: 'val_iou' reached 0.16942 (best 0.16942), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=03-val_loss=0.440-val_iou=0.17.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfcff1bc39d46fd8e70a948a9c97636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 21885: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f2b37eaa17403d898aa25159a88b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 26262: 'val_iou' reached 0.13865 (best 0.16942), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=05-val_loss=0.477-val_iou=0.14.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5964911ab96489c92c3e5e312a142a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 30639: 'val_iou' reached 0.15330 (best 0.16942), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=06-val_loss=0.467-val_iou=0.15.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40375030f9bd4a7082e8912f769bf2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 35016: 'val_iou' reached 0.15085 (best 0.16942), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=07-val_loss=0.482-val_iou=0.15.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f459bc5c200d4f2fa92bdf24ed3a9dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 39393: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df0379e90a2429a95d713c6d90dde80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 43770: 'val_iou' reached 0.15871 (best 0.16942), saving model to 'c:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\GitHub\\\\TextualSegFormer\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\transformer-epoch=09-val_loss=0.489-val_iou=0.16.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73106fe344a24264b57dd1c5de1fcbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 48147: 'val_iou' was not in top 3\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(test_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
